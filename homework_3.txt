1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?

Для многоклассовой классификации. 
При расширении двоичной метрики на задачи с несколькими классами или метками данные обрабатываются как набор двоичных задач, по одной для каждого класса. 
Затем есть несколько способов усреднить вычисления двоичных показателей по набору классов, каждый из которых может быть полезен в некотором сценарии. 
Здесь могут быть полезны различные варианты усреднения для метрик качества классификации: 

- "micro" дает каждой паре выборка-класс равный вклад в общую метрику (за исключением результата взвешивания выборки). 
Вместо того, чтобы суммировать метрику для каждого класса, это суммирует дивиденды и делители, составляющие метрики для каждого класса, 
для расчета общего частного. Микро-усреднение может быть предпочтительным в настройках с несколькими ярлыками, 
включая многоклассовую классификацию, когда класс большинства следует игнорировать; 
- "macro" просто вычисляет среднее значение двоичных показателей, придавая каждому классу одинаковый вес. 
В задачах, где редкие занятия тем не менее важны, макро-усреднение может быть средством выделения их производительности. 
С другой стороны, предположение, что все классы одинаково важны, часто неверно, так что макро-усреднение будет чрезмерно 
подчеркивать обычно низкую производительность для нечастого класса;
- "weighted" учитывает дисбаланс классов, вычисляя среднее значение двоичных показателей, 
в которых оценка каждого класса взвешивается по его присутствию в истинной выборке данных.


2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?

Алгоритмы отличаются друг от друга реализацией алгоритма boosted trees, а также технической совместимостью и ограничениями.

Например, lightgbm использует новую технику односторонней выборки на основе градиента (GOSS) для фильтрации экземпляров данных для нахождения значения разделения, 
в то время как xgboost использует предварительно отсортированный алгоритм и алгоритм на основе гистограммы для вычисления наилучшего разделения.

Для меня одной из основных особенностей является обработка категориальных признаков: 
- catboost обладает гибкостью, позволяя задавать индексы категориальных столбцов, чтобы его можно было кодировать как кодирование в одно касание. 
- Как и в catboost, lightgbm также может обрабатывать категориальные функции, вводя имена функций. Он не конвертируется в одноразовое кодирование и 
намного быстрее, чем одноразовое кодирование. lightgbm использует специальный алгоритм, чтобы найти значение разделения категориальных признаков. 
- В отличие от catboost или lightgbm, xgboost не может обрабатывать категориальные функции сам по себе, он принимает только числовые значения, подобные случайному лесу. 
Поэтому перед подачей категориальных данных в xgboost необходимо выполнить различные кодировки, такие как кодирование меток, среднее кодирование или однократное кодирование.

Если поработать со всеми алгоритмами для разных задач, можно прийти к выводу, что, к сожалению, не существует победителя по всем критериям - 
всегда нужно отталкиваться от конкретных параметров датасетов. 